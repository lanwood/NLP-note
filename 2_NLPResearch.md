### 基础性研究

1. 网络架构（Transformer）-> BERT -> 预训练模型
2. 优化理论
3. 对抗训练（提高模型稳定性），virtual Adversarial Training
4. 数据增强（增强数据与原数据分布保持一致）
5. 半监督学习
6. 域迁移（考虑加入 带标签但同训练数据来源渠道不一致的数据）
7. Meta Learning（Direct Policy Gradients vs Monte Carlo Tree Search vs AlphaZero）
8. AutoML （RENAS：遗传算法 + 增强学习）
9. 多任务学习（百度 ERNIE 2.0）
10. 集成学习
11. 图网络
12. 知识图谱（任意两个实体之间的关系）
13. 多模态学习（统一处理不同来源的数据集：图像+文本 或 结构化数据+文本）
14. 机器推理（让机器学习一个逻辑系统，实现自动推理，提高模型的泛化性，早期多用数理逻辑的方式来解决这类问题）


### 专属 NLP 领域的研究

1. 预训练预研模型（XLNet，BERT）
2. 文本分类（情绪分析，topic分类）
3. 序列标注
4. 关系提取
5. Dependency Parsing(语法解析)
6. Semantic Parsing（将自然语言转换为其他语言，例 Clever Bi -> SQL）与Lambda Calculus结合
7. Seq2Seq（T5）
8. 文本生成
9. 文本推荐
10. 翻译（Zero-Shot Translation）
11. 指代消解
12. 综合性研究
    1. 智能对话机器人
    2. 文本校对
    3. 文本检索
    4. 开源情报系统
    5. Smart Bi
